import fs from 'fs/promises';
import path from 'path';
import pipeline from './pipeline.js';
import enrich from './enrich.js';
import chalk from 'chalk';
import { convertToCsv } from './utils.js';

/**
 * Fonction principale qui orchestre l'ensemble du pipeline de scraping.
 */
async function main() {
    // 1. R√©cup√©rer les arguments de la ligne de commande
    // On ignore le premier argument si c'est '--' (ajout√© par npm)
    let args = process.argv.slice(2);
    if (args[0] === '--') args = args.slice(1);

    // Le premier argument est le nom de la source.
    const sourceName = args[0];
    // Le deuxi√®me argument (optionnel) est le mode ('test').
    const isTestMode = args[1] === 'test';

    if (isTestMode) {
        console.log("üß™ Mode test activ√©.");
        console.log("-> Nettoyage des anciens fichiers de test et logs...");
        const testDir = path.join(process.cwd(), 'data', 'test');
        try {
            // Tente de lire le contenu du dossier. S'il n'existe pas, l'erreur est captur√©e.
            const files = await fs.readdir(testDir);
            const unlinkPromises = [];
            for (const file of files) {
                // Supprime les fichiers de donn√©es et de log de test
                if ((file.startsWith(`${sourceName}-`) && file.endsWith('.test.json')) || file === `errors-${sourceName}.log`) {
                    unlinkPromises.push(fs.unlink(path.join(testDir, file)));
                }
            }
            if (unlinkPromises.length > 0) {
                await Promise.all(unlinkPromises);
            }
            console.log("-> Nettoyage termin√©.");
        } catch (err) {
            // Si le dossier n'existe pas (ENOENT), on ignore l'erreur. Sinon, on l'affiche.
            if (err.code !== 'ENOENT') {
                console.error("‚ùå Erreur lors du nettoyage des fichiers de test:", err);
            }
        }
    }

    // 2. V√©rifier si un nom de source a √©t√© fourni
    if (!sourceName) {
        console.error("‚ùå Erreur : Vous devez sp√©cifier un nom de source.");
        console.log("Exemple : node main.js frenchFab");
        process.exit(1); // Arr√™te le script avec un code d'erreur
    }

    console.log(chalk.bgBlueBright.black(`\n\n--- üöÄ D√âMARRAGE DU PIPELINE COMPLET POUR LA SOURCE : ${sourceName} ---\n`));

    try {
        // 3. Importer dynamiquement le module scraper correspondant
        const scraperPath = `./scrapers/${sourceName}.js`;
        const scraperModule = await import(scraperPath);
        const scraper = scraperModule.default; // On r√©cup√®re l'objet export√© par "export default"

        // 4. Lancer le pipeline avec la configuration dynamique
        // √âtape 1 : R√©cup√®re la liste des URLs
        await pipeline.runGetListStep(sourceName, scraper, isTestMode);

        // √âtape 2 : Scrape les d√©tails de chaque page
        await pipeline.runGetDetailsStep(sourceName, scraper, isTestMode);

        // √âtape 3a : Enrichit avec l'API SIRENE
        await enrich.enrichWithSirene(sourceName, isTestMode);

        // √âtape 3b : Enrichit avec les URLs LinkedIn
        await enrich.enrichWithLinkedIn(sourceName, isTestMode);

        // √âtape 4 : Convertit les donn√©es finales en fichiers CSV
        await convertToCsv(sourceName, isTestMode);

        console.log(chalk.bgBlueBright.black(`\n\n--- ‚úÖ PIPELINE TERMIN√â AVEC SUCC√àS POUR LA SOURCE : ${sourceName} ---\n`));

    } catch (error) {
        if (error.code === 'ERR_MODULE_NOT_FOUND') {
            console.error(chalk.red.bold(`\n\n--- ‚ùå ERREUR CRITIQUE ---`));
            console.error(`Le scraper pour la source "${sourceName}" n'a pas √©t√© trouv√©.`);
            console.error(`Veuillez v√©rifier qu'un fichier nomm√© "${sourceName}.js" existe bien dans le dossier "/scrapers".`);
        } else {
            console.error(chalk.red.bold("\n\n--- ‚ùå UNE ERREUR CRITIQUE A ARR√äT√â LE PIPELINE ---"));
            console.error(error);
        }
        process.exit(1);
    }
}

main();

// --- PISTES D'AM√âLIORATION FUTURES ---

// TODO - Parall√©lisation: Int√©grer une librairie comme 'p-limit' pour parall√©liser les √©tapes longues (ex: runGetDetailsStep, enrichWithLinkedIn) afin d'acc√©l√©rer le traitement des grosses sources.

// TODO - Gestion des Proxies: Ajouter un syst√®me de rotation de proxies (via un service externe) dans les requ√™tes fetch et Puppeteer pour √©viter les blocages d'IP lors de scraping √† grande √©chelle.

// TODO - Gestion des CAPTCHAs: Impl√©menter une solution de r√©solution de CAPTCHA (ex: 2Captcha, Anti-Captcha) pour g√©rer les blocages sur les sites qui en pr√©sentent (notamment Google/DuckDuckGo lors de la recherche LinkedIn).

// TODO - Int√©gration Google Sheets: Cr√©er une interface via Google Apps Script pour piloter le pipeline depuis une feuille de calcul. Cela impliquerait probablement de transformer ce script en une API ou un service cloud (ex: Google Cloud Function) que le Google Sheet pourrait appeler pour lancer les t√¢ches et r√©cup√©rer les r√©sultats.

// TODO - S√©curit√©: Si des cl√©s d'API ou des identifiants sont ajout√©s √† l'avenir, les d√©placer dans un fichier .env et utiliser une librairie comme 'dotenv' pour les charger, au lieu de les coder en dur.